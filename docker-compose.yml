services:
  kernel:
    build: .
    ports:
      - "${HOST_PORT_KERNEL:-5501}:8000"
    env_file:
      - path: .env
        required: false
    volumes:
      - ./models-local:/app/models-local
      - ./agent-prompts:/app/agent-prompts

  webui:
    image: node:20-alpine
    working_dir: /app
    command: sh -c "npm ci --include=dev --no-audit --no-fund && npm run dev:docker"
    ports:
      - "${HOST_PORT_WEBUI:-5500}:3000"
    environment:
      - CHOKIDAR_USEPOLLING=true
    volumes:
      - ./agent-webui:/app
      - webui_node_modules:/app/node_modules

  # ollama:
  #   # Uncomment to run Ollama as a bundled container.
  #   # By default, Ollama is expected to run natively on the host (better
  #   # performance on Apple Silicon via Metal). The kernel reaches it at
  #   # OLLAMA_BASE_URL=http://host.docker.internal:11434
  #   # GPU passthrough (--gpus all) is required for CUDA hosts.
  #   image: ollama/ollama:latest
  #   ports:
  #     - "${HOST_PORT_OLLAMA:-5504}:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama

volumes:
  webui_node_modules:
  # ollama_data:
